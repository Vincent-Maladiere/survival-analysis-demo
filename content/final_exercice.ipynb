{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92580d6d-48e5-4dee-ba2d-b549f42482f7",
   "metadata": {},
   "source": [
    "# Final exercice\n",
    "\n",
    "This exercice is a gentle recap of the concepts we have seen during the course.\n",
    "\n",
    "To submit it, please fill in your answer and download the notebook by heading to `File > Download` on the top left bar. You will then transmit the downloaded notebook to the school or to me.\n",
    "\n",
    "The two datasets we use in this notebook are the sessionization output of the `data/wowah_data_raw.parquet` dataset.\n",
    "\n",
    "**You don't need to understand how sessionization works**.\n",
    "For your information:\n",
    "- Only a light version of the input dataset, `data/wowah_data_raw.parquet_650k.parquet` is accessible here on Jupyterlite, to optimize memory and storage. It is only useful as an example to run sessionization.\n",
    "- However, the two output files `data/wowah_session_features_90days.parquet` and `data/wowah_session_events_90days.parquet` have been generated using the full version, because we prefer having more data points. We uploaded them on Jupyterlite because they are much smaller than the input dataset.\n",
    "\n",
    "A user is considered to have churned when we didn't register any activity for **90 days**. As we already have sessionized the dataset, user sessions have at least 90 days between each others, for the same user.\n",
    "\n",
    "A event is positive (value is 1) when a session has churned, it is negative (value is 0) otherwise. Here surviving means keep playing the game, without making pause longer than 90 days.\n",
    "\n",
    "*If you have any questions about this exercice or the course, you can contact me at maladiere.vincent@gmail.com*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d88316e-658b-4c39-a963-0f5ef6015741",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install setuptools fastparquet lifelines \"nbformat>=4.2.0\" seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006ab78f-4bda-43f6-81a6-8eddcec69fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns; sns.set_style(\"darkgrid\")\n",
    "\n",
    "\n",
    "X = pd.read_parquet(\"data/wowah_session_features_90days.parquet\")\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0b00c-070d-4ba2-b006-7b524b5b6026",
   "metadata": {},
   "source": [
    "Here is recap of the columns used *(you don't need to understand the race, charclass or zone)*:\n",
    "- `level`: the level of the player in the game\n",
    "- `race`: the family of the character\n",
    "- `charclass`: the job of the character\n",
    "- `zone`: the most frequent zone visited by the player\n",
    "- `guild`: the ID of the group the player belongs to, if any. If the player doesn't belong to any group, the value is `-1`.\n",
    "- `days_since_last_churn`: the number of days since last session churned\n",
    "- `days_since_first_start`: the number of days since the player first sign-up\n",
    "- `number_of_previous_churn`: the number of previous churned sessions for the player\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cbf426-b92c-4906-9c59-115e7d63a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_parquet(\"data/wowah_session_events_90days.parquet\")\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeccb6f5-e74c-46e8-9763-95370a967a9b",
   "metadata": {},
   "source": [
    "Let's observe the distribution of durations. The durations quantify the session length, whether it has churned or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad33218-cfd7-4fc2-a21b-a13cdfb665e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(y[\"duration\"], bins=52);\n",
    "ax.set(\n",
    "    title=\"Histogram of durations\",\n",
    "    xlabel=\"duration (days)\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a576b71a-3532-4dce-85c8-feceaa30cb8a",
   "metadata": {},
   "source": [
    "**Question 1: Count the number of positive (and negative) events in `y`.**\n",
    "\n",
    "*Hint: use the [.value_counts()](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html) method on `y[\"event\"]`* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16769aea-7a5f-4657-9311-cab933af426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e56774-18e4-420f-a91e-4d8868de7432",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df341d2-a832-47b9-8b4a-3c36fa76365f",
   "metadata": {},
   "source": [
    "Let's further describe `y` with a Kaplan Meier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a72d23-13f9-4072-99ad-f82774e1363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "\n",
    "km = KaplanMeierFitter()\n",
    "km.fit(durations=y[\"duration\"], event_observed=y[\"event\"])\n",
    "km.plot(title=\"Survival proba\")\n",
    "plt.ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c1097e-2c13-4178-b8ea-5057e2b86af1",
   "metadata": {},
   "source": [
    "**Question 2: By looking at this graph, what is approximately the number of days for 50% of users to churn? What is the residual survival probability?** \n",
    "\n",
    "*Hint: Verify your approximation by printing the `median_survival_time_` attribute of km.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657fc8c9-d5b7-4ec5-ad33-bc0f3384c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc4f049-4a8d-4aa8-8e7c-651381967af2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3041f3a-5f42-4d93-9af9-91792393fc15",
   "metadata": {},
   "source": [
    "Next, to start using our features `X`, we can run Kaplan Meier on a stratified feature of our choice. We need to concatenate X and y together first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d14b0-6126-43e2-903f-2ee8c3fc6efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([X, y], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0915e6-5cf7-4c7e-ba06-df2d42f2561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df[\"charclass\"] != \"Death Knight\"]\n",
    "\n",
    "for charclass, df_charclass in df.groupby(\"charclass\"):\n",
    "    \n",
    "    km = KaplanMeierFitter().fit(\n",
    "        durations=df_charclass[\"duration\"],\n",
    "        event_observed=df_charclass[\"event\"],\n",
    "    )\n",
    "    km.plot(\n",
    "        title=f\"Survival proba\",\n",
    "        label=charclass,\n",
    "    );\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b9dbe5-9a6b-4a53-b224-7c9e0733ea71",
   "metadata": {},
   "source": [
    "**Question 3: By looking at the graph above, which type of character looks to have the higher churn rate? What are the lower and higher median survival times (the durations when the survival proba is 50%)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ff370e-e8a7-4103-971f-61e8cd31fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b041454c-3a76-484e-a575-b45ba1208b3f",
   "metadata": {},
   "source": [
    "**Question 4: Run the stratified Kaplan Meier on the `race` feature instead of `charclass`, and find the category with the higher churn.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a7e7bd-71e8-44eb-988d-ac30429f8ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a2d47-90bf-4c4b-8646-dfe1f9276a61",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7193fb-0ce0-45fe-9de1-2d7fc22014d4",
   "metadata": {},
   "source": [
    "We know split the dataset between the train set and test set, and create our time grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29fa5e-6885-4a93-814f-d76cee6e8fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train_test_split_within(X, y, idx, **kwargs):\n",
    "    \"\"\"Ensure that test data durations are within train data durations.\"\"\"\n",
    "    X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(X, y, idx, **kwargs)\n",
    "    mask_duration_inliers = y_test[\"duration\"] < y_train[\"duration\"].max()\n",
    "    X_test = X_test[mask_duration_inliers]\n",
    "    y_test = y_test[mask_duration_inliers]\n",
    "    idx_test = idx_test[mask_duration_inliers]\n",
    "    return X_train, X_test, y_train, y_test, idx_train, idx_test\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split_within(\n",
    "    X, y, np.arange(X.shape[0]), test_size=0.75, random_state=0,\n",
    ")\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41970bf7-d8f2-4ec8-8f9a-0f2322fa95a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_time_grid(y_train, n_steps):\n",
    "    \"\"\"Bound times to the range of duration.\"\"\"\n",
    "    # Some survival models can fail to predict near the boundary of the\n",
    "    # range of durations observed on the training set.\n",
    "    observed_duration = y_test.loc[y_test[\"event\"] > 0][\"duration\"]\n",
    "    \n",
    "    # trim 1% of the span, 0.5% on each end:\n",
    "    span = observed_duration.max() - observed_duration.min()\n",
    "    start = observed_duration.min() + 0.005 * span\n",
    "    stop = observed_duration.max() - 0.005 * span\n",
    "    return np.linspace(start, stop, num=n_steps)\n",
    "\n",
    "\n",
    "time_grid = make_test_time_grid(y_train, n_steps=100)\n",
    "time_grid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad17d2-def8-4aec-a20e-d1262964b62c",
   "metadata": {},
   "source": [
    "Then, we define our Cox PH pipeline, using this time again the OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935245ec-aad5-425d-ad5d-6858db8a3590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline, FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "from utils import LifelinesWrapper\n",
    "\n",
    "\n",
    "categ_cols = [\"race\", \"charclass\", \"zone\"]\n",
    "\n",
    "ohe = OneHotEncoder(\n",
    "    max_categories=10,\n",
    "    sparse_output=False,\n",
    "    handle_unknown=\"infrequent_if_exist\",\n",
    ")\n",
    "\n",
    "simple_transformer = make_column_transformer(\n",
    "    (ohe, categ_cols),\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "cox_ph = LifelinesWrapper(CoxPHFitter(penalizer=0.01))\n",
    "\n",
    "cox_ph = make_pipeline(\n",
    "    simple_transformer,\n",
    "    cox_ph,\n",
    ")\n",
    "cox_ph.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2aa09-d5fa-4ab0-a251-3e269e3a4ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import SurvivalAnalysisEvaluator\n",
    "\n",
    "simple_cox_survival_probas = cox_ph.predict_survival_function(X_test, times=time_grid)\n",
    "\n",
    "evaluator = SurvivalAnalysisEvaluator(y_train, y_test, time_grid)\n",
    "evaluator(\"Simple CoxPH\", simple_cox_survival_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c8c3f4-ed1f-4b3a-8b9e-179907b65237",
   "metadata": {},
   "source": [
    "**Question 5: The `penalizer` hyper-parameter of the Cox model controls the flexibility of the model (lower is more flexible). Replace it in the code above with `1` or `10`. Does it improve the performances of the model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ef7880-35f5-475a-90c0-4e46e998cffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26aaa1-e7d2-42b2-a05e-b44a1f3b09bf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a0482-aeac-49b1-a2eb-dde879924bfd",
   "metadata": {},
   "source": [
    "Let's now switch gears and try the Gradient Boosting Incidence from hazardous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb54a369-32ff-49f0-87bb-6bff4ba6889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazardous.gradient_boosting_incidence import GradientBoostingIncidence\n",
    "\n",
    "gb_incidence = make_pipeline(\n",
    "    simple_transformer,\n",
    "    GradientBoostingIncidence(n_iter=30, max_leaf_nodes=5, learning_rate=1),\n",
    ")\n",
    "gb_incidence.fit(X_train, y_train)\n",
    "gb_incidence_survival_probas = gb_incidence.predict_survival_function(X_test, times=time_grid)\n",
    "\n",
    "evaluator(\"Gradient Boosting Incidence\", gb_incidence_survival_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46f0381-18df-43a0-9c71-fd40fcc318fd",
   "metadata": {},
   "source": [
    "**Question 6: The `learning_rate` hyper-parameter is one of the most important. It represents how fast the Gradient Boosting Incidence update its weights. Change the value of the `learning_rate` (for exemple you can choose between: `10`, `0.1`, `0.01` and `0.00001`). <br>\n",
    "Which value give better performance than the Cox?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a745fb1-f7ea-4f0f-8463-9b538862200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b109e4f-c0a6-42b3-b4b8-b8ba5bc07609",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c239574e-15a0-4833-859b-39179cff7739",
   "metadata": {},
   "source": [
    "To get a sense of the important features, we use a technique called `permutation_importance`. Broadly speaking, it compares the score (here the IBS) when we remove a column and keep the others.\n",
    "\n",
    "**This can takes up to several minutes to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38442634-95a2-45d4-a35a-e3bac3e055ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from hazardous.metrics import integrated_brier_score\n",
    "\n",
    "\n",
    "# Define the custom score to be compatible with scikit-learn (here we reuse the IBS)\n",
    "def ibs_scorer(estimator, X_test, y_test):\n",
    "    surv_probas = estimator.predict_survival_function(X_test, times=time_grid)\n",
    "    return -integrated_brier_score(y_train, y_test, surv_probas, time_grid)\n",
    "\n",
    "# ignore this\n",
    "gb_incidence[-1].set_params(show_progressbar=False)\n",
    "\n",
    "# We map our model to some name for readability \n",
    "models = {\n",
    "    \"Cox\": cox_ph,\n",
    "    \"Gradient Boosting Incidence\": gb_incidence,\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Separate the transformers from the estimator\n",
    "    transformers, estimator = model[:-1], model[-1]\n",
    "    \n",
    "    # Get the feature importance\n",
    "    results = permutation_importance(\n",
    "        estimator,\n",
    "        transformers.transform(X_test.iloc[:1000]),\n",
    "        y_test.iloc[:1000],\n",
    "        scoring=ibs_scorer,\n",
    "        n_repeats=5,\n",
    "    )\n",
    "\n",
    "    # Create the result dataframe\n",
    "    random_permutations = pd.DataFrame(\n",
    "        dict(\n",
    "            importance_mean=results.importances_mean,\n",
    "            features=transformers.get_feature_names_out(),\n",
    "        )\n",
    "    ).sort_values(\"importance_mean\", ascending=False).head(10)\n",
    "\n",
    "    # Plot!\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.barplot(\n",
    "        y=random_permutations[\"features\"],\n",
    "        x=random_permutations[\"importance_mean\"],\n",
    "        orient=\"h\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set(title=f\"Permutation Importance {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db625de-3116-42fd-b8a4-ca0c3c2dc56f",
   "metadata": {},
   "source": [
    "**Question 7: What seems to be the most important feature for both models?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc22e16-86bf-43c3-b0a5-2ed5d01587fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275bf095-6748-43a0-823b-b99166bed167",
   "metadata": {},
   "source": [
    "Finally, we use another method called **Partial Dependence** to estimate the impact of some features on the churn probability. Here we only focus on the Gradient Boosting Incidence estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867996d-df8f-4b21-9c9d-50c816e83861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "estimator = gb_incidence[-1]\n",
    "estimator.set_params(time_horizon=150, show_progressbar=False)\n",
    "\n",
    "transformers = gb_incidence[:-1]\n",
    "\n",
    "disp = PartialDependenceDisplay.from_estimator(\n",
    "    estimator,\n",
    "    transformers.transform(X_test),\n",
    "    response_method=\"predict_proba\",\n",
    "    features=[\"level\", \"number_of_previous_churn\"],\n",
    "    feature_names=transformers.get_feature_names_out(),\n",
    ")\n",
    "disp.bounding_ax_.set(title=f\"Partial dependence for Gradient Boosting Incidence\")\n",
    "ax.set(ylim=[0, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb4d55f-34f9-48af-b2f1-a2d809a2eba6",
   "metadata": {},
   "source": [
    "**Question 8: These curves represent the probability of churn, when we increase the value of one feature. What conclusions can you make for these two features?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ade848-1550-439f-8615-4482eccdb3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf110796-1c22-44ea-bcbb-49f7a70035c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
